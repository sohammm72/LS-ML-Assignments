{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9085cf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Soham Murudkar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "889394b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WikiText-2\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ec13f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4358/4358 [00:01<00:00, 3981.32 examples/s]\n",
      "Map: 100%|██████████| 36718/36718 [00:09<00:00, 3804.36 examples/s]\n",
      "Map: 100%|██████████| 3760/3760 [00:00<00:00, 3960.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], return_special_tokens_mask=True)\n",
    "\n",
    "tokenized = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples}\n",
    "    total_length = len(concatenated[\"input_ids\"])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    return {\n",
    "        k: [t[i:i+block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated.items()\n",
    "    }\n",
    "\n",
    "lm_dataset = tokenized.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dbfcb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Loss: 4.2803: 100%|██████████| 500/500 [09:01<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 3.7644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    lm_dataset[\"train\"].shuffle(seed=42).select(range(1000)),  # smaller subset for demo\n",
    "    batch_size=2,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1):  # 1 epoch for quick fine-tune\n",
    "    print(f\"\\nEpoch {epoch+1}\")\n",
    "    total_loss = 0\n",
    "    progress = tqdm(train_loader)\n",
    "    \n",
    "    for batch in progress:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Avg Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3841d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_words(prompt, model, tokenizer, num_words=10, top_k=50):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "732d3bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack and Jill went up across the River to see\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Jack and Jill went up\"\n",
    "print(generate_next_words(prompt, model, tokenizer, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74b9de1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predict(prompt):\n",
    "    return generate_next_words(prompt, model, tokenizer, num_words=3)\n",
    "\n",
    "gr.Interface(fn=predict, inputs=\"text\", outputs=\"text\", title=\"Next Word Generator\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
